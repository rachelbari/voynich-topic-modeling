{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pyLDAvis.sklearn`\n",
    "\n",
    "Modified from the pyLDAvis scikit-learn example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f13d615eef4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "import re, io, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Voynich Data\n",
    "\n",
    "Load the Pickle files produced by vms_vectorize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    index = defaultdict(str)\n",
    "    \n",
    "    with urllib.request.urlopen(data) as file:\n",
    "        for line in file.read().decode('latin-1').splitlines():\n",
    "            # pull out takahashi lines\n",
    "            m = re.match(r'^<(f.*?)\\..*;H> +(\\S.*)$', line)\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            transcription = m.group(2)\n",
    "            pg = str(m.group(1))\n",
    "\n",
    "            # ignore entire line if it has a {&NNN} or {&.} code\n",
    "            if re.search(r'\\{&(\\d|\\.)+\\}', transcription):\n",
    "                continue\n",
    "\n",
    "            # remove extraneous chracters ! and %\n",
    "            s = transcription.replace(\"!\", \"\").replace(\"%\", \"\")\n",
    "            \n",
    "            # delete all end of line {comments} (between one and three observed)\n",
    "            # ...with optional line terminator\n",
    "            # allow 0 occurences to remove end-of-line markers (- or =)\n",
    "            s = re.sub(r'([-=]?\\{[^\\{\\}]+?\\}){0,3}[-=]?\\s*$', \"\", s)\n",
    "\n",
    "            # delete start of line {comments} (single or double)\n",
    "            s = re.sub(r'^(\\{[^\\{\\}]+?\\}){1,2}', \"\", s)\n",
    "\n",
    "            # simplification: tags preceeded by -= are word breaks\n",
    "            s = re.sub(r'[-=]\\{[^\\{\\}]+?\\}', '.', s)\n",
    "\n",
    "            # these tags are nulls\n",
    "            # plant is a null in one case where it is just {plant}\n",
    "            # otherwise (above) it is a word break\n",
    "            # s = re.sub(r'\\{(fold|crease|blot|&\\w.?|plant)\\}', \"\", s)\n",
    "            # simplification: remaining tags in curly brackets\n",
    "            s = re.sub(r'\\{[^\\{\\}]+?\\}', '', s)\n",
    "\n",
    "            # special case .{\\} is still a word break\n",
    "            s = re.sub(r'\\.\\{\\\\\\}', \".\", s)\n",
    "\n",
    "            # split on word boundaries\n",
    "            # exclude null words ('')\n",
    "            words = [str(w) for w in s.split(\".\") if w]\n",
    "            paragraph = ' '.join(words).lstrip()\n",
    "            \n",
    "            index[pg] += (paragraph)\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"./models\"\n",
    "\n",
    "# load in the pickle files of stored models\n",
    "with open(\"{}/tfidf_vectorizer.pk\".format(models_path), \"rb\") as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "with open(\"{}/vms_tf.pk\".format(models_path), \"rb\") as f:\n",
    "    vms_tf = pickle.load(f)\n",
    "with open(\"{}/vms_mapping.pk\".format(models_path), \"rb\") as f:\n",
    "    vms_mapping = pickle.load(f)\n",
    "with open(\"{}/tf_vectorizer.pk\".format(models_path), \"rb\") as f:\n",
    "    tf_vectorizer = pickle.load(f)\n",
    "with open(\"{}/vms_tfidf.pk\".format(models_path), \"rb\") as f:\n",
    "    vms_tfidf = pickle.load(f)\n",
    "\n",
    "num_topics = 4\n",
    "\n",
    "index = tokenize(\"https://raw.githubusercontent.com/rachelbari/voynich-topic-modeling/master/data/text16e6.evt\")\n",
    "documents = [index[key] for key in index.keys()]\n",
    "\n",
    "#newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "#docs_raw = newsgroups.data\n",
    "#print(len(docs_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of mapping material\n",
    "map_df = pd.DataFrame(vms_mapping, columns=['folio'])\n",
    "print(map_df)\n",
    "print(vms_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Latent Dirichlet Allocation models\n",
    "\n",
    "Finally, the LDA models are fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TF\n",
    "lda_tf = LatentDirichletAllocation(n_topics=num_topics, random_state=0, max_iter=8)#, learning_method=\"online\", learning_offset=20.)\n",
    "lda_tf.fit(vms_tf)\n",
    "# for TFIDF\n",
    "lda_tfidf = LatentDirichletAllocation(n_topics=num_topics, random_state=0, max_iter=8)#, learning_method=\"online\", learning_offset=20.)\n",
    "lda_tfidf.fit(vms_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the models with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, vms_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tfidf, vms_tfidf, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using different MDS functions\n",
    "\n",
    "With `sklearn` installed, other MDS functions, such as MMDS and TSNE can be used for plotting if the default PCoA is not satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, vms_tf, tf_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, vms_tf, tf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
